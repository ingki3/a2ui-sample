# A2UI (Agentic AI User Interface)

A2UIëŠ” LLM(Large Language Model)ì˜ Tool Calling ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤(UI)ë¥¼ ìƒì„±í•˜ê³  ì œì–´í•˜ëŠ” Agentic UI í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
ì„œë²„ ì¤‘ì‹¬(Server-Driven) UI ì„¤ê³„ë¥¼ í†µí•´, AIê°€ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•œ ë’¤, ê·¸ ê²°ê³¼ë¥¼ ì‚¬ì „ì— ì •ì˜ëœ UI ì»´í¬ë„ŒíŠ¸ë¡œ ë Œë”ë§í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **LLM Tool Calling ê¸°ë°˜ UI ìƒì„±**: ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ í•´ì„í•˜ì—¬ ì ì ˆí•œ í•¨ìˆ˜(Tool)ë¥¼ ì‹¤í–‰í•˜ê³ , ê·¸ ê²°ê³¼ì— ë§ëŠ” UIë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
- **Server-Driven UI (SDUI)**: ì„œë²„ì—ì„œ JSON í˜•íƒœë¡œ UI êµ¬ì¡°ë¥¼ ì •ì˜í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡í•©ë‹ˆë‹¤. í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ë¥¼ ë Œë”ë§ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.
- **ë©€í‹° ì¸í…íŠ¸(Multi-Intent) ì§€ì›**: "ì• í”Œ ì£¼ê°€ë‘ ê°•ë‚¨ì—­ ë§›ì§‘ ì•Œë ¤ì¤˜"ì™€ ê°™ì€ ë³µí•© ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ê³ , ì—¬ëŸ¬ UI ë¸”ë¡ì„ ë™ì‹œì— ë Œë”ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ìŠ¤íŠ¸ë¦¬ë° ì½”ë©˜í„°ë¦¬**: ë°ì´í„° ì‹œê°í™”(ì°¨íŠ¸ ë“±)ì™€ í•¨ê»˜ AIì˜ ë¶„ì„ ë‚´ìš©ì„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. í™˜ê²½ ì„¤ì •

`uv` ë˜ëŠ” `pip`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv .venv
source .venv/bin/activate  # Mac/Linux

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  í•„ìš”í•œ API Keyë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.

```env
GOOGLE_API_KEY=your_gemini_api_key
NAVER_CLIENT_ID=your_naver_client_id  # ë§›ì§‘/ì‡¼í•‘ ê²€ìƒ‰ìš©
NAVER_CLIENT_SECRET=your_naver_client_secret
```

### 3. ì„œë²„ ì‹¤í–‰

```bash
uvicorn app.api.main:app --reload
```

---

## ğŸ§© Tool Calling ê¸°ë°˜ í˜¸ì¶œ í”„ë¡œì„¸ìŠ¤ (ìƒì„¸ ì„¤ëª…)

A2UIì˜ í•µì‹¬ì€ **"LLMì˜ íŒë‹¨ -> ë„êµ¬ ì‹¤í–‰ -> UI ë Œë”ë§"** ìœ¼ë¡œ ì´ì–´ì§€ëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

### 1. í”„ë¡œì„¸ìŠ¤ ê°œìš”

```mermaid
sequenceDiagram
    participant User
    participant Server (FastAPI)
    participant LLM (Gemini)
    participant Service (Agent)
    
    User->>Server: "ì‚¼ì„±ì „ì ì£¼ê°€ ë³´ì—¬ì¤˜"
    Server->>LLM: process_query(text)
    LLM-->>Server: Tool Call ìš”ì²­ (get_stock_chart, symbol="005930.KS")
    Server->>Service: get_stock_chart("005930.KS") í˜¸ì¶œ
    Service-->>Server: A2UI JSON ì‘ë‹µ (Chart ì»´í¬ë„ŒíŠ¸ í¬í•¨)
    Server-->>User: SSE Event (A2UI JSON + Streaming Text)
```

### 2. ì½”ë“œ ë ˆë²¨ ìƒì„¸ ì„¤ëª…

#### Step 1: ë„êµ¬(Tool) ì •ì˜ (`app/services/llm_wrapper.py`)

LLMì—ê²Œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ì„ JSON Schema í˜•íƒœë¡œ ì•Œë ¤ì¤ë‹ˆë‹¤.

```python
# llm_wrapper.py

auth_tool = {
    "function_declarations": [
        {
            "name": "get_stock_chart",
            "description": "Get the stock price history chart for a given symbol.",
            "parameters": {
                "type": "OBJECT",
                "properties": {
                    "symbol": {
                        "type": "STRING",
                        "description": "The stock symbol (e.g. AAPL, GOOG)."
                    }
                },
                "required": ["symbol"]
            }
        },
        # ... ë‹¤ë¥¸ ë„êµ¬ë“¤ (find_restaurants, calculate_loan ë“±)
    ]
}

self.model = genai.GenerativeModel(
    model_name='gemini-2.0-flash',
    tools=[auth_tool],
    # ...
)
```

#### Step 2: LLMì˜ ì˜ë„ íŒŒì•… ë° ë„êµ¬ í˜¸ì¶œ ë°˜í™˜ (`app/services/llm_wrapper.py`)

ì‚¬ìš©ìì˜ ìì—°ì–´ ì…ë ¥ì„ ë°›ì•„ LLMì´ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.

```python
# llm_wrapper.py

def process_query(self, text: str) -> Dict[str, Any]:
    response = self.chat.send_message(text)
    
    # LLM ì‘ë‹µì—ì„œ function_call íŒŒì‹±
    tool_calls = []
    if response.parts:
        for part in response.parts:
            if fn := part.function_call:
                tool_calls.append({
                    "tool_name": fn.name,
                    "tool_args": dict(fn.args)  # ì˜ˆ: {"symbol": "AAPL"}
                })
    
    if tool_calls:
        return {
            "type": "multiple_tool_calls",
            "calls": tool_calls
        }
    # ...
```

#### Step 3: ë„êµ¬ ì‹¤í–‰ ë° ë¼ìš°íŒ… (`app/api/main.py`)

ì„œë²„ëŠ” LLMì´ ìš”ì²­í•œ ë„êµ¬ ì´ë¦„(`tool_name`)ì„ í™•ì¸í•˜ê³ , ì‹¤ì œ ì„œë¹„ìŠ¤ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
# main.py (chat_stream í•¨ìˆ˜ ë‚´ë¶€)

if processed["type"] == "multiple_tool_calls":
    for call in processed["calls"]:
        tool_name = call["tool_name"]
        args = call["tool_args"]
        
        if tool_name == "get_stock_chart":
            # ì‹¤ì œ ì„œë¹„ìŠ¤ í˜¸ì¶œ
            res = stock_service.get_stock_chart(args.get("symbol"))
            
        elif tool_name == "find_restaurants":
            # ì‹¤ì œ ì„œë¹„ìŠ¤ í˜¸ì¶œ
            res = restaurant_service.find_restaurants(args.get("location"), args.get("cuisine"))
            
        # ... í˜¸ì¶œ ê²°ê³¼(res)ëŠ” A2UIResponse ê°ì²´
```

#### Step 4: UI ìƒì„± ë° Jinja2 í…œí”Œë¦¿ ë Œë”ë§ (`app/services/agent.py`)

ì„œë¹„ìŠ¤ ë¡œì§ì€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ í›„, ë¯¸ë¦¬ ì •ì˜ëœ **Jinja2 í…œí”Œë¦¿**ì— ë°ì´í„°ë¥¼ ì£¼ì…í•˜ì—¬ A2UI JSONì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
# agent.py

def get_stock_chart(self, symbol: str) -> Union[A2UIResponse, TextResponse]:
    # 1. ë°ì´í„° ì¡°íšŒ (yfinance ë“±)
    ticker = yf.Ticker(symbol)
    hist = ticker.history(period="1y")
    # ... ë°ì´í„° ê°€ê³µ ...

    # 2. í…œí”Œë¦¿ ë Œë”ë§ (UI êµ¬ì¡° ìƒì„±)
    return self._render_template("stock_chart.json.j2", {
        "symbol": symbol.upper(),
        "prices": prices,
        "current_price": f"${hist['Close'].iloc[-1]:.2f}"
    })
```

**í…œí”Œë¦¿ ì˜ˆì‹œ (`stock_chart.json.j2`)**:
```jinja2
{
  "surfaceUpdate": {
    "components": [
      {
        "id": "{{ uid }}_chart",
        "component": {
          "Chart": {
            "data": {{ prices | tojson }},
            "color": "#0F9D58"
          }
        }
      }
    ]
  }
}
```

#### Step 5: í´ë¼ì´ì–¸íŠ¸ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡ (`app/api/main.py`)

ìƒì„±ëœ A2UI ë°ì´í„°ëŠ” SSE(Server-Sent Events)ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.

```python
# main.py

# A2UI ì´ë²¤íŠ¸ ì „ì†¡
if res and isinstance(res, A2UIResponse):
    yield f"event: a2ui\ndata: {json.dumps(res.model_dump())}\n\n"

# (ì˜µì…˜) LLM ì½”ë©˜í„°ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì „ì†¡
async for chunk in llm.generate_commentary_stream(...):
    yield f"event: text\ndata: {json.dumps({'text': chunk})}\n\n"
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
a2ui/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py          # FastAPI ì„œë²„ ë° ì—”ë“œí¬ì¸íŠ¸
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ agent.py         # ì‹¤ì œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ì„œë¹„ìŠ¤ ë¡œì§ (Tools)
â”‚   â”‚   â””â”€â”€ llm_wrapper.py   # Gemini LLM ì—°ë™ ë° Function Calling ì²˜ë¦¬
â”‚   â”œâ”€â”€ schemas/             # Pydantic ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ templates/           # UI ì»´í¬ë„ŒíŠ¸ í…œí”Œë¦¿ (Jinja2)
â”œâ”€â”€ static/                  # í´ë¼ì´ì–¸íŠ¸ ì •ì  íŒŒì¼ (HTML, JS Renderer)
â”œâ”€â”€ A2UI_implementation.md   # ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ
â””â”€â”€ requirements.txt         # í”„ë¡œì íŠ¸ ì˜ì¡´ì„±
```
